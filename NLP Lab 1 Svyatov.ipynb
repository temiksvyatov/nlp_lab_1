{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3 style=\"color: red;\"><b>IMPORTANT</b></h3>\n",
    "> \n",
    "> Я в третий раз буду говорить о том, что данный проект лучше всего запускать в специализированной среде, чтобы не устанавливать лишних зависимостей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Оглавление\n",
    "<details>\n",
    "<summary>Развернуть оглавление</summary>\n",
    "\n",
    "1. Описываем задачу\n",
    "2. Импортируем зависимости\n",
    "3. Подготовка необходимого инструментария\n",
    "4. EDA Анализ\n",
    "5. Кодируем и оцениваем\n",
    "6. Обучаем \n",
    "7. Производим оценку модели\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Описываем задачу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Задача: классификация текста`\n",
    "\n",
    "Требуется классифицировать твиты по чемпионату мира по футболу 2022 года. Всего есть 3 класса: positive, negative and neutral.\n",
    "\n",
    "<details>\n",
    "<summary>Данные</summary>\n",
    "Date Created - дата\n",
    "Number of Likes - кол-во лайков\n",
    "Source of Tweet - источник твита\n",
    "Tweet - сам твит\n",
    "Sentiment - метка класса\n",
    "</details>\n",
    "\n",
    "**Что надо сделать:**\n",
    "\n",
    "1. EDA, data processing ( графики, статистика, обработка данных, какие то выводы по вашим данным)\n",
    "2. Classic ML approaches (Не менее 3-х)(Logistic regression, SVM etc. )\n",
    "3. DL approaches (Не менее 2-х) (FC, RNN, CNN 1d, 2d, LSTM). Предобученные брать можно, трансформеры пока брать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng3TX46a6z1F"
   },
   "source": [
    "## 2. Импортируем зависимости\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Roz8LSD4k2Xm"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8aIbT8IMi4z",
    "outputId": "4e0fcbc4-30ca-46ff-ed65-da399074fa69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjjTbnRYMjT6",
    "outputId": "3c1fa6d7-5c68-46cc-d8d0-1d4053053ae2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n06_YrGBMlO8"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-9Pxjka7Ekt"
   },
   "source": [
    "## 3. Подготовка необходимого инструментария\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wl1VN8JuMnfo"
   },
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\\\W+', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return [word for word in text if word not in string.punctuation]\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatization(text):\n",
    "    lem = nltk.stem.WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = [word for word in text if not word.isnumeric()]\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_short_words(text, min_len=3):\n",
    "    text = [word for word in text if len(word) > min_len]\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_long_words(text, max_len=15):\n",
    "    text = [word for word in text if len(word) < max_len]\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_empty(text):\n",
    "    text = [word for word in text if word != '']\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_space(text):\n",
    "    text = [word.strip() for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cQukrsbMtqR"
   },
   "outputs": [],
   "source": [
    "class Preprocess(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=True, tokenization=True, remove_punctuation=True, remove_stopwords=True, stemming=True, lemmatization=True, remove_numbers=True, remove_short_words=True, remove_long_words=True, remove_empty=True, remove_space=True, remove_specific_words=False):\n",
    "        self.lower = lower\n",
    "        self.tokenization = tokenization\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        self.lemmatization = lemmatization\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_short_words = remove_short_words\n",
    "        self.remove_long_words = remove_long_words\n",
    "        self.remove_empty = remove_empty\n",
    "        self.remove_space = remove_space\n",
    "        self.remove_specific_words = remove_specific_words\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.lower:\n",
    "            X = X.apply(lambda x: lower(x))\n",
    "        if self.tokenization:\n",
    "            X = X.apply(lambda x: tokenization(x))\n",
    "        if self.remove_punctuation:\n",
    "            X = X.apply(lambda x: remove_punctuation(x))\n",
    "        if self.remove_stopwords:\n",
    "            X = X.apply(lambda x: remove_stopwords(x))\n",
    "        if self.stemming:\n",
    "            X = X.apply(lambda x: stemming(x))\n",
    "        if self.lemmatization:\n",
    "            X = X.apply(lambda x: lemmatization(x))\n",
    "        if self.remove_numbers:\n",
    "            X = X.apply(lambda x: remove_numbers(x))\n",
    "        if self.remove_short_words:\n",
    "            X = X.apply(lambda x: remove_short_words(x))\n",
    "        if self.remove_long_words:\n",
    "            X = X.apply(lambda x: remove_long_words(x))\n",
    "        if self.remove_empty:\n",
    "            X = X.apply(lambda x: remove_empty(x))\n",
    "        if self.remove_space:\n",
    "            X = X.apply(lambda x: remove_space(x))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_kOo9hjMyHv"
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocess', Preprocess())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahrCYRo59FvS"
   },
   "source": [
    "`Через Processes будем конвеерно автоматизировать обработку текста.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y51_TprL7eof"
   },
   "source": [
    "## 4. EDA анализ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sZsoDia57Ra"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_data.csv', index_col=0)\n",
    "df_submission = pd.read_csv('test_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO5ejfzDPQoK"
   },
   "outputs": [],
   "source": [
    "X = df['Tweet'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVpX6rWuOi88"
   },
   "outputs": [],
   "source": [
    "clean_text = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWWXIiMXPcj-"
   },
   "outputs": [],
   "source": [
    "clean_text_str = clean_text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UncBjr03Pggh"
   },
   "outputs": [],
   "source": [
    "df['text'] = clean_text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOnmmvaDwBAG"
   },
   "outputs": [],
   "source": [
    "df['class_text'] = pd.Categorical(df['Sentiment'])\n",
    "df['class'] = df['class_text'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_J5Eta28Q2C",
    "outputId": "f1f05992-0c77-43df-de57-2cb829ac5918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    7539\n",
       "neutral     7372\n",
       "negative    5089\n",
       "Name: class_text, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGZLZoEWjnS1"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'xgboost': xgb.XGBClassifier()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt8scEKelucJ"
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H81KG4H17xds"
   },
   "source": [
    "## 5. Кодируем и оцениваем\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQecywUQ9Shu"
   },
   "source": [
    "`Для более комфортной работы превратим метки из текста в число, и через кросс-валидацию выберем лучшую модель, опираясь на полученные метрики.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhQ2YPc0qDMx",
    "outputId": "db1ec07b-22c9-4ab3-9b94-4ac79afecc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression | Vectorizer: tfidf | F1 Micro Cross-Validation Scores: [0.7035  0.6905  0.70525 0.68425 0.70525]\n",
      "Average F1 Micro Score: 0.69775\n",
      "Model: logistic_regression | Vectorizer: count | F1 Micro Cross-Validation Scores: [0.68925 0.68175 0.6975  0.6855  0.7    ]\n",
      "Average F1 Micro Score: 0.6908000000000001\n"
     ]
    }
   ],
   "source": [
    "# Custom transformer for Word2Vec\n",
    "\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.size = 100\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [row.split() for row in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.size, window=5, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n",
    "                    or [np.zeros(self.size)], axis=0)\n",
    "            for words in [row.split() for row in X]\n",
    "        ])\n",
    "\n",
    "# Custom transformer for Doc2Vec\n",
    "\n",
    "\n",
    "class Doc2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.size = 100\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tagged_data = [TaggedDocument(words=row.split(), tags=[str(i)]) for i, row in enumerate(X)]\n",
    "        self.model = Doc2Vec(tagged_data, vector_size=self.size, window=2, min_count=1, workers=4, epochs=20)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.infer_vector(row.split()) for row in X])\n",
    "\n",
    "# BERT Embeddings - Using previously defined bert_encode function\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "word2vec_vectorizer = Word2VecVectorizer()\n",
    "doc2vec_vectorizer = Doc2VecVectorizer()\n",
    "\n",
    "f1_micro_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "vectorizers = {\n",
    "    'tfidf': tfidf_vectorizer,\n",
    "    'count': count_vectorizer\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for vec_name, vectorizer in vectorizers.items():\n",
    "        if vec_name != 'bert':\n",
    "            pipeline = make_pipeline(vectorizer, model)\n",
    "            scores = cross_val_score(pipeline, X, y, cv=5, scoring=f1_micro_scorer)\n",
    "            print(f\"Model: {model_name} | Vectorizer: {vec_name} | F1 Micro Cross-Validation Scores: {scores}\")\n",
    "            print(f\"Average F1 Micro Score: {np.mean(scores)}\")\n",
    "            avg_score = np.mean(scores)\n",
    "\n",
    "            results[(model_name, vec_name)] = avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOQ-pClk73Qj"
   },
   "source": [
    "## 6. Обучаем\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcnHdJ8t9Vtm"
   },
   "source": [
    "`Для прогона наших данных определим несколько моделек, которые будем обучать.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8KqLfw8s9RK",
    "outputId": "bb93c2ac-8a6c-4fe4-dd4c-87ddfdf6afb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: logistic_regression | Vectorizer: tfidf | F1 Micro Score: 0.69775\n"
     ]
    }
   ],
   "source": [
    "best_model, best_vec = max(results, key=results.get)\n",
    "best_score = results[(best_model, best_vec)]\n",
    "\n",
    "print(f\"Best Model: {best_model} | Vectorizer: {best_vec} | F1 Micro Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jxlfY4mxGwS"
   },
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('/content/test_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pPqIfG19w_n"
   },
   "source": [
    "`Для возможного дальнейшего анализа мы подготавливаем наши тестовые данные через лучшего конкурсанта (модель) и запихиваем все в файл.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2EXn0OwyCsD"
   },
   "outputs": [],
   "source": [
    "X_clean = pipe.transform(X)\n",
    "X_clean_str = X_clean.apply(lambda x: ' '.join(x))\n",
    "\n",
    "best_vectorizer = vectorizers[best_vec].fit(X_clean_str)\n",
    "X_vectorized = best_vectorizer.transform(X_clean_str)\n",
    "\n",
    "best_model_instance = models[best_model].fit(X_vectorized, y)\n",
    "\n",
    "X_submission = df_submission['Tweet'].copy()\n",
    "X_submission_clean = pipe.transform(X_submission)\n",
    "X_submission_clean_str = X_submission_clean.apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_submission_vectorized = best_vectorizer.transform(X_submission_clean_str)\n",
    "\n",
    "predictions = best_model_instance.predict(X_submission_vectorized)\n",
    "\n",
    "label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "predicted_labels = [label_mapping[label] for label in predictions]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': df_submission.index,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "submission_df.to_csv('/content/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
